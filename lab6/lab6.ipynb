{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import functools\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antrenarea unei retele neurale multi-strat presupune rabdare si un orizont larg de cunostinte. Este mai degraba \"black magic\" decat programare. Codul vostru poate fi perfect, insa reteaua nu invata nici de frica. Asa incat ati fost avertizati!\n",
    "\n",
    "Antrenarea cere o combinatie de pricepere, setare de parametri si initializare de ponderi. In cele ce urmeaza am sa incerc sa va ghidez in asa fel incat sa ajungeti la rezultatul final, si anume la o acuratete de test de peste 95%. \n",
    "\n",
    "Va rog neaparat sa cititi cartea lui Michael Nielsen, http://neuralnetworksanddeeplearning.com, pentru ca doar prin programare si suportul de curs nu veti reusi sa ajustati parametrii retelei. Trebuie sa intelegeti cum functioneaza reteaua pentru ca sa o puteti antrena cu succes.\n",
    "\n",
    "Un blog interesant este si https://mmlind.github.io/Simple_3-Layer_Neural_Network_for_MNIST_Handwriting_Recognition/; l-am folosit pentru setarea parametrilor, insa performantele obtinute sunt mai bune decat scrie acolo.\n",
    "\n",
    "Revin inca o data: ca sa antrenati reteaua cu succes trebuie sa:\n",
    " * scrieti codul fara erori;\n",
    " * initializati ponderile intr-un anumit fel;\n",
    " * folositi anumiti parametri de antrenare.\n",
    " \n",
    "Altfel RETEAUA NU INVATA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(file_name):\n",
    "    \"\"\"\n",
    "    Returns a list of dictionaries, each element of the list is an instance\n",
    "    \"\"\"\n",
    "    # ...\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def form_x_and_y(dataset):\n",
    "    \"\"\"\n",
    "    Forms the dataframe as numpy arrays from the given list of dictionaries\n",
    "    \"\"\"\n",
    "    # ...\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalization means division by 255 -> values in [0, 1]\n",
    "    \"\"\"\n",
    "    return x / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_sample(data, label):\n",
    "    \"\"\"\n",
    "    Shows an instance as a 2D image\n",
    "    \"\"\"\n",
    "    # Make those columns into a array of 8-bits pixels\n",
    "    # The pixel intensity values are integers from 0 to 255\n",
    "    pixels = np.array(data, dtype='uint8')\n",
    "\n",
    "    # Reshape the array into 28 x 28 array (2-dimensional array)\n",
    "    n = int(np.sqrt(len(pixels)))\n",
    "    assert n**2 == len(pixels)\n",
    "    pixels = pixels.reshape(n, n)\n",
    "\n",
    "    # Plot\n",
    "    plt.title('Label is {label}'.format(label=label))\n",
    "    plt.imshow(pixels, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "    #return np.tanh(z)\n",
    "\n",
    "# este posibil sa testam ca atat functia sigmoida cat si functia tanh() sunt calculate corect\n",
    "# folositi testul pentru ca altfel erorile sunt greu, daca nu imposibil de gasit!\n",
    "assert(sigmoid(0) == 0.5)\n",
    "\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "    #return 1.0 - (np.tanh(z) ** 2)\n",
    "\n",
    "assert(sigmoid_derivative(0) == 0.25)\n",
    "# idem, faceti teste pentru tanh(). Valoarea tanh(0) trebuie sa fie 0.\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    # ...\n",
    "\n",
    "# definiti testele pentru softmax si folositi-le\n",
    "assert(np.all(softmax( np.array([[5], [5]]) ) == np.array([[0.5], [0.5]])))\n",
    "assert(np.all(softmax( np.array([[0], [0]]) ) == np.array([[0.5], [0.5]])))\n",
    "assert(np.sum(softmax(np.array([[4], [2]]))) == 1.0)\n",
    "\n",
    "\n",
    "def one_hot(p, k):\n",
    "    \"\"\" One-hot encoding converteste o eticheta, de exemplu 7, in [0 0 0 0 0 0 0 1 0 0]\n",
    "        parametrul p este pozitia pe care se pune \"1\"\n",
    "        parametrul k este numarul total de clase (elemente in lista)\n",
    "        Am facut conventia ca one_hot sa intoarca o lista\n",
    "    \"\"\"\n",
    "    # ...\n",
    "    \n",
    "assert(one_hot(0, 2) == [1, 0])\n",
    "assert(one_hot(2, 4) == [0, 0, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first, load the datasets\n",
    "train_set = load_dataset('mnist_train.csv')\n",
    "test_set = load_dataset('mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/GaZcvjNMuXwm2Xq/wCc0Up6OKHNfAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbac9a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# definiti numarul de clase\n",
    "k = 10\n",
    "\n",
    "x_train, y_train = form_x_and_y(train_set)\n",
    "assert np.shape(x_train) == (len(train_set), 28*28)\n",
    "assert np.shape(y_train) == (len(train_set), 1)\n",
    "\n",
    "y_train = np.array([one_hot(int(y), k) for y in y_train])\n",
    "assert np.shape(y_train) == (len(train_set), k)\n",
    "\n",
    "x_test, y_test = form_x_and_y(test_set)\n",
    "assert np.shape(x_test) == (len(test_set), 28*28)\n",
    "assert np.shape(y_test) == (len(test_set), 1)\n",
    "\n",
    "y_test = np.array([one_hot(int(y), k) for y in y_test])\n",
    "assert np.shape(y_test) == (len(test_set), k)\n",
    "\n",
    "# inspectati vizual un sample, sa fie ce va asteptati\n",
    "show_sample(x_train[1], y_train[1])\n",
    "\n",
    "x_train = normalize(x_train)\n",
    "x_test = normalize(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# la inceput, am folosit problema XOR ca sa antrenez o retea cu 3 straturi,\n",
    "# cu neuroni cu functie de activare sigmoida (inclusiv pe ultimul strat)\n",
    "# structura aleasa a fost cu functie de cost patratica (nu cross-entropy)\n",
    "# si cu neuroni cu functie de activare sigmoida\n",
    "# \n",
    "# rata de invatare a fost 4.0 si a trebuit sa fac in jur de 4000 de epoci, cu antrenare \n",
    "# de tip stochastic gradient descent (update dupa fiecare pattern), pana sa clasifice \n",
    "# cu acuratete de 100%\n",
    "#\n",
    "# in acest caz, setul de test este totuna cu setul de antrenare \n",
    "x_train = np.array([\n",
    "    [0.3, 0.3],\n",
    "    [0.7, 0.7],\n",
    "    [0.3, 0.7],\n",
    "    [0.7, 0.3]\n",
    "])\n",
    "y_train = np.array([\n",
    "    [0],\n",
    "    [0],\n",
    "    [1],\n",
    "    [1]\n",
    "])\n",
    "y_train = np.array([one_hot(int(x), 2) for x in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assert_func(weights, biases, sizes):\n",
    "    \"\"\" Functia are rolul de a verifica corectitudinea arhitecturii retelei\n",
    "    \"\"\"\n",
    "    assert(len(biases) == len(weights))\n",
    "    assert(len(biases) == len(sizes) - 1)\n",
    "    assert(len(weights) == len(sizes) - 1)\n",
    "    for i in range(len(biases)):\n",
    "        assert(biases[i].shape[0] == sizes[i + 1])\n",
    "        assert(biases[i].shape[1] == 1)\n",
    "        assert(weights[i].shape[0] == sizes[i + 1])\n",
    "        assert(weights[i].shape[1] == sizes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sizes este o lista care pastreaza numarul de neuroni de pe fiecare strat, pornind de la intrare\n",
    "#sizes = [2, 3, 2] # pentru XOR \n",
    "sizes = [28 * 28, 100, 10]\n",
    "layers = len(sizes)\n",
    "\n",
    "# strategia aleasa a fost de a construi intai reteaua cu functii de activare sigmoida pentru XOR\n",
    "# pentru acest tip de retea, ponderile au fost initializate aleator (uniform) cu valori cuprinse intre -1 si 1\n",
    "\n",
    "# ulterior, functiile de activare au fost schimbate la tanh() cu exceptia celor de pe ultimul strat unde \n",
    "# s-a folosit softmax(), iar functia de cost a fost schimbata din patratul diferentei la cross-entropy\n",
    "\n",
    "# pentru aceasta strunctura, initializarea corecta in asa fel incat neuronii sa nu se satureze, este CRUCIALA\n",
    "# a se vedea http://neuralnetworksanddeeplearning.com/chap3.html#weight_initialization\n",
    "# daca nu initializati ponderile cum trebuie, puteti sa va confruntati cu situatia ca reteaua NU INVATA DELOC\n",
    "\n",
    "# deci puteti folosi initializarea asa cum este descrisa de Glorot in http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi\n",
    "# (aveti link si la curs, [26] Xavier Glorot, Yoshua Bengio, \"Understanding the difficulty of training\n",
    "# deep feedforward neural networks\"). Puteti sa le initializati si cu valori mai mici decat este descris acolo, asta pare sa ajute.\n",
    "\n",
    "# for every layer, the bias vector has the same dimension as the number of neurons\n",
    "# ...\n",
    "\n",
    "# modul de lucru: scrieti o functie si verificati apoi ca functioneaza pe un pattern (pentru asta aveti XOR, ca sa puteti\n",
    "# urmari totul pas cu pas).\n",
    "# ATENTIE: nu scrieti tot codul deodatat fara sa verificati nimic decat la sfarsit; sigur nu va merge si eroarea va fi greu de gasit.\n",
    "\n",
    "# verificati corectitudinea arhitecturii construite\n",
    "assert_func(weights, biases, sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9497\n",
      "9635\n",
      "9649\n",
      "9556\n",
      "9650\n",
      "9692\n",
      "9638\n",
      "9650\n",
      "9685\n",
      "9694\n",
      "9716\n",
      "9680\n",
      "9722\n",
      "9677\n",
      "9671\n",
      "9712\n",
      "9677\n",
      "9649\n",
      "9663\n",
      "9712\n",
      "9709\n",
      "9716\n",
      "9703\n",
      "9708\n",
      "9654\n",
      "9694\n",
      "9729\n",
      "9664\n",
      "9716\n",
      "9723\n"
     ]
    }
   ],
   "source": [
    "def feedforward(x, weights, biases):\n",
    "    \"\"\" Propagarea inainte folosind neuroni cu functie de activare sigmoida peste tot \"\"\"\n",
    "    # ...\n",
    "\n",
    "assert(feedforward(x_train[0], weights, biases).shape[0] == sizes[-1])\n",
    "\n",
    "\n",
    "def feedforward2(x, weights, biases):\n",
    "    \"\"\" Propagarea inainte folosind neuroni cu functie de activare softmax pe ultimul strat \"\"\"\n",
    "    # ...\n",
    "\n",
    "assert(feedforward2(x_train[0], weights, biases).shape[0] == sizes[-1])\n",
    "\n",
    "\n",
    "def error(x_set, y_set, weights, biases):\n",
    "    \"\"\" Calculeaza iesirea sub forma de one-hot encoding folosind feedforward(),\n",
    "        si o foloseste pentru a numara cate pattern-uri sunt antrenate corect\n",
    "    \"\"\"\n",
    "    # ...\n",
    "\n",
    "\n",
    "def error2(x_set, y_set, weights, biases):\n",
    "    \"\"\" Calculeaza iesirea sub forma de one-hot encoding folosind feedforward2()\n",
    "        si o foloseste pentru a numara cate pattern-uri sunt antrenate corect\n",
    "    \"\"\"\n",
    "    # ...\n",
    "\n",
    "\n",
    "def backprop(x, y, weights, biases, layers):\n",
    "    # for the forward pass, we would compute activations and nets (the z-s)\n",
    "    # for every layer, since we would use it in the backward pass (delta calculation)\n",
    "    ...\n",
    "\n",
    "    # for the cross-entropy function, be sure to change the activation\n",
    "    # and delta calculatios for the last layer\n",
    "    \n",
    "    # forward pass computes activations\n",
    "    ...\n",
    "        \n",
    "\n",
    "    # declare deltas for the backward pass\n",
    "    ...    \n",
    "    \n",
    "    # first do the computations for the last layer\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    # start with the layer before last\n",
    "    ...\n",
    "            \n",
    "    return delta_w, delta_b\n",
    "\n",
    "\n",
    "def compute_batch(batch_x, batch_y, weights, biases, eta, lmbda, layers):    \n",
    "    \"\"\" Recomputes the weights and biases using only a batch \"\"\"\n",
    "    \"\"\" make use of the assert_func to verify the dimensionalities \"\"\"\n",
    "    delta_w = ...\n",
    "    delta_b = ...\n",
    "    assert_func(delta_w, delta_b, sizes)\n",
    "    \n",
    "    for ... :\n",
    "        dw, db = backprop(x, y, weights, biases, layers)\n",
    "        assert_func(dw, db, sizes)\n",
    "        delta_b = ...\n",
    "        delta_w = ...\n",
    "        assert_func(delta_w, delta_b, sizes)\n",
    "        \n",
    "    # here we have the update of the weights, computed using the learning rate eta,\n",
    "    # and the regularization factor lambda\n",
    "    # for starters, you can put lambda to 0 (for XOR problem, just to see it is working)\n",
    "    # anyway, you should choose a small lambda (< 0.01 or even smaller)\n",
    "    weights = ...\n",
    "    biases = ...\n",
    "    \n",
    "    # before returning, call the assert function\n",
    "    # after you are convinced the code is correct, comment out the asserts,\n",
    "    # because these incur great performance penalties\n",
    "    assert_func(weights, biases, sizes)\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "# for the MNIST, batch size is 10; for XOR, it is 1\n",
    "\n",
    "# For the XOR problem, be warned that the correct classification may be attained even only after more than 4000 epochs\n",
    "# eta = 4.0, epochs = 2000\n",
    "\n",
    "...\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    perm = [i for i in range(x_train.shape[0])]\n",
    "    np.random.shuffle(perm)\n",
    "    \n",
    "    # generate batches\n",
    "    x_batches = ...\n",
    "    y_batches = ...\n",
    "\n",
    "    # run batches and change weights\n",
    "    for ... :\n",
    "        weights, biases = compute_batch(batch_x, batch_y, weights, biases, eta, lmbda, layers)\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    print( error(x_test, y_test, weights, biases) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 97.23%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa9aaa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Accuracy is %3.2f%%\" % (error(x_test, y_test, weights, biases) / 100))\n",
    "y = feedforward2(x_test[147], weights, biases)\n",
    "show_sample(x_test[147]*255, np.argmax(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
